{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d760d81",
   "metadata": {},
   "source": [
    "# MA506 Probability and Statistical Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfb87916",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Covariance of y is sigma^2I; H transpose is symmetric. H transpose is H; HH^T = H^2 are both also H; cov(e)= sigma^2(I-H)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f32509e",
   "metadata": {},
   "source": [
    "## Condition for a regression model to be a linear regression model:\n",
    "1. Linear relationship between y and x\n",
    "2. Fixed variance across all points\n",
    "3. Each data point is independent from the next\n",
    "4. Errors are normally distributed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c8b1e1",
   "metadata": {},
   "source": [
    "# 1. Extending to a more general cases of Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec541732",
   "metadata": {},
   "source": [
    "## 1.1 Instead of 4 samples, we have n samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958b98f1",
   "metadata": {},
   "source": [
    "Modify the X, Y  matrices to incorporate all the samples: $(x_1,y_1),(x_2,y_2),...,(x_n,y_n)$\n",
    "\n",
    "$$\n",
    "Y = \\begin{bmatrix}\n",
    "y_1\\\\\n",
    "y_2\\\\\n",
    "\\vdots\\\\\n",
    "y_n\n",
    "\\end{bmatrix};\n",
    "\\quad\n",
    "X = \\begin{bmatrix}\n",
    "1 & x_1\\\\\n",
    "1 & x_2\\\\\n",
    "\\vdots & \\vdots\\\\\n",
    "1 & x_n\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "And the optimal coefficients $\\hat{\\beta}$ will have the same expression as before (4) with updated X and Y. Hence still we will have\n",
    "\n",
    "$$\\hat{\\beta} = (X^TX)^{-1}X^TY$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2502020",
   "metadata": {},
   "source": [
    "## 1.2 Instead of $y = \\beta_0 + \\beta_1x$ we want : $y = \\beta_0 + \\beta_1x + \\beta_2x^2+...+\\beta_mx^m$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4fb6c5",
   "metadata": {},
   "source": [
    "Assuming we still have n data points, relative to the base case, now we need to update all matrices involved: $X$, $Y$ and $\\beta$:\n",
    "\n",
    "$$\n",
    "Y = \\begin{bmatrix}\n",
    "y_1\\\\\n",
    "y_2\\\\\n",
    "\\vdots\\\\\n",
    "y_n\n",
    "\\end{bmatrix};\n",
    "\\quad\n",
    "X = \\begin{bmatrix}\n",
    "1 & x_1 & x_1^2 & \\cdots & x_1^m\\\\\n",
    "1 & x_2 & x_2^2 & \\cdots & x_2^m\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "1 & x_n & x_n^2 & \\cdots & x_n^m\n",
    "\\end{bmatrix};\n",
    "\\quad\n",
    "\\beta = \n",
    "\\begin{bmatrix}\n",
    "\\beta_0\\\\\n",
    "\\beta_1\\\\\n",
    "\\vdots\\\\\n",
    "\\beta_m\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "With these new matrices, the optimal coefficients (estimate of $\\beta$: $\\hat{\\beta}$) will still have the same expression as (4)\n",
    "\n",
    "$$\\hat{\\beta} = (X^TX)^{-1}X^TY$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb83ea1f",
   "metadata": {},
   "source": [
    "## 1.3 Instead of $y = \\beta_0 + \\beta_1x$ we want : $y = \\beta_0 + \\beta_1 sin(x) + \\beta_2 cos(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231ad874",
   "metadata": {},
   "source": [
    "Assuming $n$ samples, again updating $X$, $Y$ and $\\beta$ as follows\n",
    "\n",
    "$$\n",
    "Y = \\begin{bmatrix}\n",
    "y_1\\\\\n",
    "y_2\\\\\n",
    "\\vdots\\\\\n",
    "y_n\n",
    "\\end{bmatrix};\n",
    "\\quad\n",
    "X = \\begin{bmatrix}\n",
    "1 & Sin(x_1) & Cos(x_1)\\\\\n",
    "1 & Sin(x_2) & Cos(x_2)\\\\\n",
    "\\vdots & \\vdots & \\vdots\\\\\n",
    "1 & Sin(x_n) & Cos(x_n)\n",
    "\\end{bmatrix};\n",
    "\\quad\n",
    "\\beta = \n",
    "\\begin{bmatrix}\n",
    "\\beta_0\\\\\n",
    "\\beta_1\\\\\n",
    "\\beta_2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Like before, with updated $X$, $Y$ and $\\beta$, we still have: $$\\hat{\\beta} = (X^TX)^{-1}X^TY$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9473894",
   "metadata": {},
   "source": [
    "## 1.4: Linear regression in multiple dimensions (Multiple Linear Regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b95e9c",
   "metadata": {},
   "source": [
    "Suppose we have n samples in d dimensions. Hence each sample has d coordinates. For instance, $i^{th}$ sample is represented as follows\n",
    "$$\n",
    "(X_i,y_i) = (x_{i1},x_{i2},...,x_{id},y_i) \n",
    "$$\n",
    "Please note that $y_i$ is still 1-dimensional.\n",
    "\n",
    "Now, suppose we wish to fit a hyperplane of the following form in $d$ dimensions. Hence for a given $X = (x_1,x_2,..,x_d)$ it should be able to predict a $y$ value using the following equation\n",
    "\n",
    "$$\n",
    "y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ...+ \\beta_dx_d\n",
    "$$\n",
    "\n",
    "In order to fit this hyperplane, we will again have a new definition of $X$, $Y$ and $\\beta$ as follows:\n",
    "\n",
    "$$\n",
    "Y = \\begin{bmatrix}\n",
    "y_1\\\\\n",
    "y_2\\\\\n",
    "\\vdots\\\\\n",
    "y_n\n",
    "\\end{bmatrix};\n",
    "\\quad\n",
    "X = \\begin{bmatrix}\n",
    "1 & x_{11} & x_{12} & \\cdots & x_{1d}\\\\\n",
    "1 & x_{21} & x_{22} & \\cdots & x_{2d}\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "1 & x_{n1} & x_{n2} & \\cdots & x_{nd}\\\\\n",
    "\\end{bmatrix};\n",
    "\\quad\n",
    "\\beta = \n",
    "\\begin{bmatrix}\n",
    "\\beta_0\\\\\n",
    "\\beta_1\\\\\n",
    "\\vdots\\\\\n",
    "\\beta_d\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "And $\\beta$ will again be inferred as: $$\\hat{\\beta} = (X^TX)^{-1}X^TY$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a59b60",
   "metadata": {},
   "source": [
    "Higher degree polynomials etc can also be incorporated directly into the model as before when doing multiple linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62daca9f",
   "metadata": {},
   "source": [
    "# 2 Characterizing a linear regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238cd282",
   "metadata": {},
   "source": [
    "## 2.1 Summarizing discussed forms of linear regression models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4132866",
   "metadata": {},
   "source": [
    "With $X$, $Y$ and $\\beta$ representing covariates, observations and parameters/weights, any regression model that can be expressed in the form:\n",
    "\n",
    "$$Y = X\\beta + \\epsilon, \\quad \\epsilon \\sim N(0,\\sigma^2I) \\tag{5}$$\n",
    "\n",
    "is a linear model with an estimate of $\\beta: \\hat{\\beta} = (X^TX)^{-1}X^TY$. Assuming n to be the number of samples, here $Y \\in \\mathbb{R}^n$, $X \\in \\mathbb{R}^{nx(p+1)}$, $\\beta \\in \\mathbb{R}^{(p+1)}$, $\\sigma^2 \\in \\mathbb{R}^{+}$ and $I$ is an n-dimensional identity matrix. Please note that the residual vector: $[e_1, e_2, e_3, e_4]^T$ discussed in the experiments at the beginning of this notebook is just a realization of $\\epsilon$ in this general model formulation. \n",
    "\n",
    "$\\textbf{Hence, all the models discussed above belong to the general family of Linear Regression models}$ with separate definitions for $X$, $Y$ and $\\beta$ as shown in the below mentioned cases. $\\color{red}{\\text{Please note X matrix formulated for fitting a regression model is generally referred to as the design matrix or model matrix}}$.\n",
    "1. $\\textbf{Case 1}$: n 1-dimensional (univariate) samples and fitting a straight line model.\n",
    "$$\n",
    "Y = \\begin{bmatrix}\n",
    "y_1\\\\\n",
    "y_2\\\\\n",
    "\\vdots\\\\\n",
    "y_n\n",
    "\\end{bmatrix};\n",
    "\\quad\n",
    "X = \\begin{bmatrix}\n",
    "1 & x_1\\\\\n",
    "1 & x_2\\\\\n",
    "\\vdots & \\vdots\\\\\n",
    "1 & x_n\n",
    "\\end{bmatrix};\n",
    "\\quad\n",
    "\\beta = \\begin{bmatrix}\n",
    "\\beta_0\\\\\n",
    "\\beta_1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "Here p = 1.\n",
    "2. $\\textbf{Case 2}$: n 1-dimensional (univariate) samples and fitting $y = \\beta_0 + \\beta_1x + \\beta_2x^2+...+\\beta_mx^m$\n",
    "$$\n",
    "Y = \\begin{bmatrix}\n",
    "y_1\\\\\n",
    "y_2\\\\\n",
    "\\vdots\\\\\n",
    "y_n\n",
    "\\end{bmatrix};\n",
    "\\quad\n",
    "X = \\begin{bmatrix}\n",
    "1 & x_1 & x_1^2 & \\cdots & x_1^m\\\\\n",
    "1 & x_2 & x_2^2 & \\cdots & x_2^m\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "1 & x_n & x_n^2 & \\cdots & x_n^m\n",
    "\\end{bmatrix};\n",
    "\\quad\n",
    "\\beta = \n",
    "\\begin{bmatrix}\n",
    "\\beta_0\\\\\n",
    "\\beta_1\\\\\n",
    "\\vdots\\\\\n",
    "\\beta_m\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "Here p = m.\n",
    "3. $\\textbf{Case 3}$: n 1-dimensional (univariate) samples and fitting $y = \\beta_0 + \\beta_1 sin(x) + \\beta_2 cos(x)$\n",
    "$$\n",
    "Y = \\begin{bmatrix}\n",
    "y_1\\\\\n",
    "y_2\\\\\n",
    "\\vdots\\\\\n",
    "y_n\n",
    "\\end{bmatrix};\n",
    "\\quad\n",
    "X = \\begin{bmatrix}\n",
    "1 & Sin(x_1) & Cos(x_1)\\\\\n",
    "1 & Sin(x_2) & Cos(x_2)\\\\\n",
    "\\vdots & \\vdots & \\vdots\\\\\n",
    "1 & Sin(x_n) & Cos(x_n)\n",
    "\\end{bmatrix};\n",
    "\\quad\n",
    "\\beta = \n",
    "\\begin{bmatrix}\n",
    "\\beta_0\\\\\n",
    "\\beta_1\\\\\n",
    "\\beta_2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "Here p = 2.\n",
    "4. $\\textbf{Case 4}$: n d-dimensional (multivariate) samples and fitting $y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ...+ \\beta_dx_d$\n",
    "$$\n",
    "Y = \\begin{bmatrix}\n",
    "y_1\\\\\n",
    "y_2\\\\\n",
    "\\vdots\\\\\n",
    "y_n\n",
    "\\end{bmatrix};\n",
    "\\quad\n",
    "X = \\begin{bmatrix}\n",
    "1 & x_{11} & x_{12} & \\cdots & x_{1d}\\\\\n",
    "1 & x_{21} & x_{22} & \\cdots & x_{2d}\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "1 & x_{n1} & x_{n2} & \\cdots & x_{nd}\\\\\n",
    "\\end{bmatrix};\n",
    "\\quad\n",
    "\\beta = \n",
    "\\begin{bmatrix}\n",
    "\\beta_0\\\\\n",
    "\\beta_1\\\\\n",
    "\\vdots\\\\\n",
    "\\beta_d\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "Here p = d."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02782e17",
   "metadata": {},
   "source": [
    "## 2.2 What makes these regression models linear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddc0745",
   "metadata": {},
   "source": [
    "Hence, for a regression model to be linear, following conditions should be satisfied:\n",
    "1. $\\textbf{Linearity}$: Y and X should be related linearly. One good way to check it is that the model should have the ability to be expressed as a matrix vector product with coeffcients ($\\beta$) separable from covariate matrix X.\n",
    "2. $\\textbf{Homoscedasticity}$: The variance of residual is the same for any value of X.\n",
    "3. $\\textbf{Independence}$: Observations are indendent of each other.\n",
    "4. $\\textbf{Normality}$: For any fixed value of X, Y is normally distributed.\n",
    "\n",
    "$\\textbf{Please note}$: For the assumed model definiton in (5) (mentioned below as well) all 4 requirements for a linear regression model are satisfied:\n",
    "\n",
    "$$Y = X\\beta + \\epsilon, \\quad \\epsilon \\sim N(0,\\sigma^2I)$$\n",
    "\n",
    "1. The product $X\\beta$ ensures 1 is satisfied (linearity)\n",
    "2. Error term $\\epsilon$ being additive to $X\\beta$ with a distribution $N(0,\\sigma^2I)$ gurantees (2), (3) and (4) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b70606a",
   "metadata": {},
   "source": [
    "## 2.3 Non-linear regression models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f614541",
   "metadata": {},
   "source": [
    "All the models that don't satisfy the linear model specific conditions are a kind of a non-linear regression model. For example Neural Network models. These models dont have closed form expressions for parameters/weigths (like $\\hat{\\beta} = (X^TX)^{-1}X^TY$ for linear models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00c6466",
   "metadata": {},
   "source": [
    "## <mark style=\"background-color: #FFFF00\">Exercise</mark> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7528fa1d",
   "metadata": {},
   "source": [
    "1. Generate data from $f(x) = Sin(x)+log(x)$ for $x \\in [0,10]$\n",
    "2. Find the weights $\\hat{\\beta}$ for fitting 4 separate models: \n",
    "- $y = \\beta_0 + \\beta_1 Sin(x)$\n",
    "- $y = \\beta_0 + \\beta_1 Cos(x)$, \n",
    "- $y = \\beta_0 + \\beta_1 Sin(x) + \\beta_2 log(x)$\n",
    "- $y = \\beta_0 + \\beta_1 Cos(x) + \\beta_2 log(x)$\n",
    "\n",
    "\n",
    "3. Predict f(x) for x = 5.7, using the learnt weights $\\hat{\\beta}$ for each model separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8999b338",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
